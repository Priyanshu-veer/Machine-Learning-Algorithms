# Random Forest Regression with Synthetic Data

## ğŸ“Œ Project Overview
This project demonstrates how to build and evaluate a **RandomForestRegressor** using synthetic data generated by `sklearn.make_regression`.
It also shows how to detect and reduce overfitting using hyperparameter tuning.


## ğŸ“Š Dataset
The dataset is generated using:
- `n_samples = 1000`
- `n_features = 20`
- `n_targets = 1`
- `random_state = 42`

The data contains both informative and noisy features, which makes it useful
for studying overfitting behavior.


## âš™ï¸ Model Used
- **RandomForestRegressor**
- Initial model trained with manually selected hyperparameters
- Hyperparameter tuning performed using **GridSearchCV**


## ğŸ§ª Evaluation Metrics
- **RÂ² Score**
- Comparison of:
  - Training RÂ²
  - Testing RÂ²
- Overfitting analysis


## ğŸ” Results

### Before Hyperparameter Tuning
- Training RÂ²: ~0.98
- Testing RÂ²: ~0.65â€“0.70

### After GridSearchCV
- Training RÂ²: ~0.97
- Testing RÂ²: ~0.77

This shows reduced overfitting and improved generalization.


## ğŸ› ï¸ Libraries Used
- Python
- NumPy
- scikit-learn


## ğŸ“ˆ Key Learnings
- Random Forest models can easily overfit on noisy data
- Limiting tree depth and leaf size reduces overfitting
- GridSearchCV helps find optimal hyperparameters
- Synthetic data is useful for understanding model behavior


## ğŸš€ Future Improvements
- Add cross-validation based evaluation
- Compare with linear models like Ridge and Lasso
- Visualize learning curves
